ğŸ† Universal Sports Data Pipeline

ğŸš€ Overview

A scalable, automated pipeline that processes sports streaming data from multiple suppliers into a unified, clean format. The system automatically fetches, standardizes, merges, and delivers sports match data with streaming links.

ğŸ“ Complete File Structure

ğŸ—‚ï¸ Root Directory

text
/
â”œâ”€â”€ ğŸ”„ universal-update.yml          # GitHub Actions workflow (main pipeline)
â”œâ”€â”€ ğŸ“Š master-data.json              # FINAL OUTPUT: Clean, merged sports data
â”œâ”€â”€ ğŸ—ï¸ standardization-UNIVERSAL.json # Phase 1 output: Standardized supplier data
â””â”€â”€ ğŸ“š README.md                     # This documentation
ğŸ”§ Scripts Directory (/scripts/)

text
/scripts/
â”œâ”€â”€ ğŸ”„ update-suppliers.js           # Fetches fresh data from all APIs
â”œâ”€â”€ ğŸ—ï¸ universal-standardizer.js     # Phase 1: Standardizes all supplier data
â”œâ”€â”€ ğŸ¯ phase2-processor.js           # Phase 2: Merges duplicates & creates master
â””â”€â”€ ğŸ§ª test-suppliers.js             # Testing utility
ğŸ“¦ Suppliers Directory (/suppliers/)

text
/suppliers/
â”œâ”€â”€ ğŸ”„ update-results.json           # Update status & metrics
â”œâ”€â”€ ğŸ“¥ tom-data.json                 # Raw data from Tom API
â”œâ”€â”€ ğŸ“¥ sarah-data.json               # Raw data from Sarah API  
â”œâ”€â”€ ğŸ“¥ wendy-data.json               # Raw data from Wendy API
â””â”€â”€ ğŸ“‚ backups/                      # Auto-managed backups (24hr retention)
    â”œâ”€â”€ tom-data-1764161975295.json
    â”œâ”€â”€ sarah-data-1764161974314.json
    â””â”€â”€ wendy-data-1764148624101.json
ğŸ§© Modules Directory (/modules/)

text
/modules/
â””â”€â”€ ğŸ—ºï¸ normalization-map.js          # Universal field mapping & sports classification
ğŸ”„ Pipeline Flow

Step 1: Data Collection

File: update-suppliers.js

Fetches from 3 suppliers: Tom, Sarah, Wendy
Circuit Breakers prevent API spam during failures
Auto-backup before each update
Auto-recovery uses backups if APIs fail
24-hour cleanup deletes old backups automatically
Step 2: Phase 1 - Standardization

File: universal-standardizer.js

Input: Raw supplier data (tom-data.json, sarah-data.json, wendy-data.json)
Process: Converts all data to common format using normalization-map.js
Output: standardization-UNIVERSAL.json (2346 matches)
Step 3: Phase 2 - Advanced Processing

File: phase2-processor.js

Input: Standardized data from Phase 1
Process: Smart merging, deduplication, stream consolidation
Output: master-data.json (1504 matches - 36% compression)
ğŸ¯ Master Data Structure

Clean, Minimal Format:

javascript
{
  "unix_timestamp": 1764161975,      // Standardized time
  "sport": "Basketball",             // Classified sport name
  "tournament": "NBA",               // League/tournament
  "match": "Lakers vs Celtics",      // Standard team format
  "sources": {                       // Streams organized by supplier
    "tom": ["https://topembed.pw/..."],
    "sarah": ["https://embedsports.top/..."],
    "wendy": ["https://spiderembed.top/..."]
  },
  "confidence": 0.8,                 // Merge confidence score
  "merged": true,                    // Whether match was merged
  "merged_count": 3                  // Number of sources merged
}
ğŸ§  Smart Features

ğŸ” Field Normalization

File: normalization-map.js

Universal Mapping: Handles 20+ field name variations automatically
Sport Classification: 50+ sports with intelligent detection
Team Formatting: Converts all to "Team A vs Team B" format
Timestamp Standardization: Handles multiple time formats
ğŸ¤ Intelligent Merging

Low Thresholds: Merges at 20-25% similarity (aggressive deduplication)
Cross-Supplier: Combines matches from Tom + Sarah + Wendy
Stream Consolidation: Preserves all unique streaming links
Quality Preservation: Maintains source attribution
ğŸ›¡ï¸ Resilience Features

Circuit Breakers: Prevents API spam during outages
Auto-Backup: Creates backups before each update
Auto-Recovery: Restores from backup if APIs fail
24-hour Cleanup: Prevents repository bloat
ğŸš€ GitHub Actions Pipeline

File: universal-update.yml

yaml
name: ğŸ”„ Universal Data Pipeline
on:
  schedule:
    - cron: '*/60 * * * *'  # Every 60 minutes
  workflow_dispatch:        # Manual trigger

jobs:
  universal-pipeline:
    steps:
      - ğŸ“¥ Checkout
      - ğŸ”„ Update Suppliers      # update-suppliers.js
      - ğŸ—ï¸ Phase 1 Standardization # universal-standardizer.js  
      - ğŸ¯ Phase 2 Processing    # phase2-processor.js
      - âœ… Verify Output
      - ğŸ“ Commit Results
ğŸ“Š Performance Metrics

Typical Results:

Input: 2,300+ raw matches from suppliers
Output: 1,500+ clean matches (36% compression)
Processing: < 10 seconds
Memory: < 10MB
Wendy Integration: 200-500 streams included
ğŸ¯ Adding New Suppliers

Simple 2-Step Process:

Add to supplier config:
javascript
// In update-suppliers.js
{
  name: 'newsupplier',
  urls: ['https://api.newsupplier.com/data'],
  processor: (data) => { /* conversion logic */ }
}
Field mapping happens automatically via normalization-map.js
No code changes needed! The universal system auto-detects field names.

ğŸ”§ Key Technologies

Node.js - Runtime environment
GitHub Actions - Automation & scheduling
Circuit Breaker Pattern - API resilience
Field Normalization - Universal data mapping
Fuzzy Matching - Intelligent deduplication
ğŸ‰ Benefits

Scalable: Ready for 20+ suppliers without code changes
Resilient: Auto-recovery from API failures
Efficient: 36% data compression through smart merging
Clean: Minimal, well-structured output
Fast: Complete pipeline in under 10 seconds
Maintainable: Clear separation of concerns
ğŸš€ The universal sports data pipeline is production-ready and scaling beautifully!
